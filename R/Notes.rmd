---
title: "Notes"
author: "Peter Nash"
date: "18/10/2020"
output: pdf_document
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
```

## Notes




## Lecture 3 Notes

For ML we will always need to specify the distribution. We are asking what value of theta given the observations we have seen from an assumed/known distribution. 

How does the information matrix relate to the Variance? The information matrix is the second derivative of the the log likelihood, so the slope of the slope, if we have determined the slope and that in itself is not steep then it is hard to pinpoint where our solution lies, (thinking along 2d lines if we have a line of very small slope any small change in y can lead to a large change in X so hard to pinpoint our variable, if it is steeper a small change in y will lead to a small change in x so easier to estimate our paramater) so if the second derivative is small, or information martix is small, then the variance in our estimate of our paramater can be large. If the second derivative is large then the information matrix is large and variance in our estimator is less. 

The ML estimator is normally distributed and is efficient. 



Log Likelihood: We are assuming independence here in our original notes, hence the joint distribution is the product of their marginal distributions. This is not the case for a time-series but we will look at this later. (Assume a product of their conditional distributions and marginals)

## Questions


The example in Log likelihood $L(\theta) = f(y_1, \theta)f(y_2, \theta)f(y_T, \theta)$, we are assuming each sample is independent, 
what if this is not the case? e.g. if $y_t$ is not independent of $y_{t-1}$, e.g. if we take the price of an equity this is not 
indpendent, but if we take the return this would be independent.

# Tutorial 2 Covering Log Likelihood

We are choosing the value of theta which maximises our probability density function. Higher joint probability 
distribution , higher probability, implies more likely

https://www.youtube.com/watch?v=XepXtl9YKwc&vl=en&ab_channel=StatQuestwithJoshStarmer

towardsdatascience.com/maximum-likelihood-estimation-explained-normal-distribution-6207b322e47f

arg max notation, the argument that maximises $arg max_{\theta E \Theta}$
## Lecture 5 Notes

Looking at testing and inference. Could this estimate have happened by chance. We will look at distributions of the estimators.

If the p-value is small it is unlikely that the estimate was generated by the true value

p-value think about it as the probability the null hypothesis is true

why is individual null hypthoses a t-test and join and f-test?, how is this linked to what we saw ,

Joint and individual tests can conflict

trying to link the linear restrictions we impose with what 


so our restrictions are our hypotheses. So if $\beta$ is a kx1 vector of parameters then $R\beta = q$ is just like $\beta_0 = u$,  $\beta_1 - v$, $\beta_2 = w$

QL 

## Live Lecture 19/10/2020 of lecture 4

Log Likelihood: We are assuming independence here in our original notes, hence the joint distribution is the product of their marginal distributions. This is not the case for a time-series but we will look at this later
--
Thinking of the null hypothesis, our starting point that this is true,  if this is true and we reject it this is type 1, if this is false and we accept it then this is type 2, so type 1 is getting our starting point wrong, type two is accepting when it is false
--
t-ratio distributed around 0 with a t-distribution if the null hypothesis is true

--- 
We do the exact test only with normal errors and linear restrictions. 
We do this if we want to test hypothesis or to test our estimators.

So with the exact test we know the exact distribution rather than using an asymptotic distribution e.g. normal. For example a standard normal we would be looking at += 1.96 where as a t-distribution would be += 2.04, this means that we would be less likely to reject the null hypothesis. And remember as samples increase the t-distribution becomes more like the normal 



in $R \beta = q$ we are doing this purely for the algebra. 

Example Slide: $q = [0, 1]$ is the vector that allows $\beta_2 =1$ and
$\beta_3 = -\beta_4$. Multiply out the vectors to see. 


---
Use BIC model selection criteria in general.


--- 
Exams will contain output from a statistical programmes and we will be asked questions on this. 


## Lecture 6 Asymptotic Test Proceedures. 

If we can't get exact results we look at asymptotic approximations, we are more likely to reject using this 

use the F where possible when both F & $\chi^2$ are given as the later is asymptotic and F is exact

$S(\hat\theta)$, the score vector is the derivative of the LL w.r.t each of the k elements (number of co-efficents/parameters) of the vector $\theta$

$\lambda^' R(\theta)$ is a scalar

Q: why are we doing this restriction?

The restrictions are our hypotheses, if they are trye, the two log likelihood should be similar $LL(\theta) - LL(\theta^\star)$, See lecture 5 above

Q: Maybe speak about how the hypotheses and restrictions are alike

- Test Procedures
$R(\hat\theta)$ has the same structure as $R\hat{\beta} - q$
  
*Note: The difference between Legrange multiplier is that the variance covariance matrix in the Wald is evaluated at $\hat\theta$ versus $\theta^{\star}$ 

- Source of non linear restrictions

Q: What is the restricted and non restricted Model?

Is $\rho$ the correlation co-efficient?

Where doe our hypothesis $H_0: R_m(\theta) = \theta_1\theta_2 - \theta_3 =0$ originate from?

-- Bootstrap

Q: THis can be applied to any of the estimators we've used?


Bootrap food for small sample estimates

ARDL model is what we look at in week 3 of the practicals. 


## Live Lecture 23/10/2020 (lecture 6)

Restrictions  Hypotheses are the same things, 

So tip: Start from a general model and then begin to impose restrictions

e.g Start with normal LRM, but if we observe some autocorrelation then model this

See Source of non linear restrictions 

so see $y_t = \beta x_t -\rho\beta x_{t-1} + \rho y_{t-1}$

we can say $\alpha = \rho$ and $ \beta_1 = \alpha  \beta_0$

equation 3 is our restricted model, 
equation 2 is our unrestricted, if we impose a restriction we reduce the number of variables

Non-linear estimation

Why is it non linear? We are multiplying $\rho$ and $\beta$, multiplicative affect

--
in final equation, generalised least squares. 

$y_t - \rho y_{t-1}$ is on the left hand side as we know this $y_t$ and $y_{t-1}$ and we make an initial estimate of \rho

-- 
The serial correlation of the errors implies that the previous value of the variable affects the current value


-- Recommended reading

'A guide to Econometrics' KEnnedy
Stock and Watson - from Ron
Verbeek - closest to the course
