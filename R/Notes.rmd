---
title: "Notes"
author: "Peter Nash"
date: "18/10/2020"
output: pdf_document
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
```

## Notes




## Lecture 3 Notes

For ML we will always need to specify the distribution. We are asking what value of theta given the observations we have seen from an assumed/known distribution. 

How does the information matrix relate to the Variance? The information matrix is the second derivative of the the log likelihood, so the slope of the slope, if we have determined the slope and that in itself is not steep then it is hard to pinpoint where our solution lies, (thinking along 2d lines if we have a line of very small slope any small change in y can lead to a large change in X so hard to pinpoint our variable, if it is steeper a small change in y will lead to a small change in x so easier to estimate our paramater) so if the second derivative is small, or information martix is small, then the variance in our estimate of our paramater can be large. If the second derivative is large then the information matrix is large and variance in our estimator is less. 

The ML estimator is normally distributed and is efficient. 



Log Likelihood: We are assuming independence here in our original notes, hence the joint distribution is the product of their marginal distributions. This is not the case for a time-series but we will look at this later. (Assume a product of their conditional distributions and marginals)

## Questions


The example in Log likelihood $L(\theta) = f(y_1, \theta)f(y_2, \theta)f(y_T, \theta)$, we are assuming each sample is independent, 
what if this is not the case? e.g. if $y_t$ is not independent of $y_{t-1}$, e.g. if we take the price of an equity this is not 
indpendent, but if we take the return this would be independent.


## Lecture 5 Notes

Looking at testing and inference. Could this estimate have happened by chance. We will look at distributions of the estimators.

If the p-value is small it is unlikely that the estimate was generated by the true value

p-value think about it as the probability the null hypothesis is true

why is individual null hypthoses a t-test and join and f-test?, how is this linked to what we saw ,

Joint and individual tests can con√°ict

trying to link the linear restrictions we impose with what 

## Live Lecture 19/10/2020 of lecture 4

Log Likelihood: We are assuming independence here in our original notes, hence the joint distribution is the product of their marginal distributions. This is not the case for a time-series but we will look at this later
--
Thinking of the null hypothesis, our starting point that this is true,  if this is true and we reject it this is type 1, if this is false and we accept it then this is type 2, so type 1 is getting our starting point wrong, type two is accepting when it is false
--
t-ratio distributed around 0 with a t-distribution if the null hypothesis is true

--- 
We do the exact test only with normal errors and linear restrictions. 
We do this if we want to test hypothesis or to test our estimators.

So with the exact test we know the exact distribution rather than using an asymptotic distribution e.g. normal. For example a standard normal we would be looking at += 1.96 where as a t-distribution would be += 2.04, this means that we would be less likely to reject the null hypothesis. And remember as samples increase the t-distribution becomes more like the normal 



in $R \beta = q$ we are doing this purely for the algebra. 

Example Slide: $q = [0, 1]$ is the vector that allows $\beta_2 =1$ and
$\beta_3 = -\beta_4$. Multiply out the vectors to see. 


---
Use BIC model selection criteria in general.


--- 
Exams will contain output from a statistical programmes and we will be asked questions on this. 

