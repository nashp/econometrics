---
title: "Revision (ReadingWeek)"
author: "Peter Nash"
date: "09/11/2020"
output: pdf_document
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
```

## Week 1/Lecture 1

We begin with the linear regression model 
$y_t = \beta_0 + \beta_1 x_t + u_t$

We will look at three ways of estimating it

1) Method of moments
2) Least Squares
3) Maximum Likelihood

We need to make assumptions. 

1) $E(u_t) = 0$, zero mean
2) We assume the errors are of constant variance or homoskedastic, $E(u^2_t) = \sigma^2$
3) We assume the errors are not autocorrelated $E(u_t u_{t-i})$
4) We require $X_t$ to vary and are exogenous i.e $X_t%$ and $u_t$ are not correlated
This implies $E(u_t) = 0$ and $E(X_t u_t) = 0$

First method of moments

So we can use our two assumptions in 4 to give two equations for two unknowns. We substitute
in $u_t = y_t - \beta_0 _ \beta_1 x_t$ into the two assumptions in 4 and solve for 
$\beta_0$ and $\beta_1$

Now Least Squares:

So least squares aims to find the values that minimise $S = \Sigma_t \hat{u}^2_t$

*Note*: Look at differentiation here

Again substitute in $u_t = y_t - \beta_0 _ \beta_1 x_t$. Multiply out then we
differentiate $S$ with respect to $\beta$

Properties: 

The estimator is unbiased 

i.e. $E(\hat{\beta}) = \beta$

We want to show

$E(\hat{\beta} - \beta) = 0$

We look get $\hat{\beta} - \beta = 0$, take expectation and work from there. 
we use our  assumptions that $E(u_t) = 0$ and $E(X_t u_t) = 0$, exogeneity

We show the variance is

$V(\hat{\beta}) = \sigma^2/\Sigma_t x_t$

## Week 1/Lecture 2

Now we look at matrix algebra and multiple regression

so we have 

$y = X \beta + u$ where $y$ is T x 1,$X$ is T x k and $u$ is T x 1

and now our assumptions $E(u^2_t) = \sigma^2$

this becomes $E(u u') = \sigma^2 I$, if one multiplies out $u u'$ this becomes apparent
and we can use $E(u^2_t) = \sigma^2$ and $E(u_t u_{t-i}) = 0$

Our assumptions: 

No exact multicollinearity, if it is then X is not full rank (it should be of rank k) and
we cannot take the inverse of $X'X$, it is non singular. 

Our exogeneity assumption is $E(X'u) = 0$. 

The explanatory variables in X are either
1) non stochastic
2) strictly exogenous, ie independent of the errors
3) pre-determined, uncorrelated

if 1 & 2 hold then the estimator is unbiased.
if 3 holds then it is biased but consistent ie as the sample increases the bias decreases

For Method of Moments we use  $E(X'u) = 0$, plug in $y - X\hat{\beta}$ for $u$ and solve

*Note* Check how $E(X'u) = 0$ is like $X'\hat{u} = 0$
Similarly for Least Squares

We find $\hat\beta$ that minimises $\hat u' \hat u$ . Substitute in again.

Remember to look at the items that are scalars, these can be switched around 
$y'X\hat\beta = \hat\beta'X'y$
. 

Quadratic form, this is $x' A x$

We get something of the form $y' y - 2\hat\beta X' y + \hat\beta X'X \hat\beta$. 
Taking the derivative of the last term is like taking the derivative of a quadritic so we will get
$2 X' X \hat \beta$, where is the transpose? this is constructed so we will have a row vector of betas
otherwise it would not work. It gives us a k x 1 vector, it looks like we are taking the derivative of $\beta'$ but we are not. See notes.Take second derivative $2 X'X$is positive definite so this is a minimum.

Check if the estimator is biased again. Similar as before using our exogoneity assumptions. 

If the exogoneity assumption fails then it is not unbiased. 

Variance: 

$V(\hat\beta) = E(\hat\beta - E(\hat\beta))(\hat\beta - E(\hat\beta))'$ work out

$E(\hat\beta - \beta)(\hat\beta - \beta)' = \sigma^2(X'X)^{-1}$ We use our assumption that 
$E(u'u) =0$, homogeneity. If not then we will have our $\Omega$ in the middle here. Like
$\sigma^2 (X'X)^{-1}X'\Omega X(X'X)^{-1}$ This heterogenous or heteresketastic which means our estimator is 
unbiased bit it is not minimum variance i.e. not efficient

Remember we estimate $V(\hat\beta)= s^2(X'X)^{-1}$ where $s^2 = \hat u' \hat u/(T-k)$.

*Note* If you can't fix the serial correlation use robust standard errors

Now Predicted values and residuals

$\hat u = y - X \hat \beta = My$ where $P_x = X(X'X)^{-1}X' ; M = I - P_x $ where P is our 
projection matrix. 

*Note* need to find out what this is exactly for

but it allows us to estimate $\sigma^2$ because $E(\hat u' \hat u) = \sigma^2(T-k)$ so 
$\sigma^2 = E(\hat u' u) / (T-k)$, and take the expectation of both sides, expectation of an expectation 
is a constant so we will get $\sigma^2 = \hat u'\hat u / (T-k)$

Goodness of fit

$R^2$ will always improve as you add a variable, standard error much better

Gauss-Markov Theorem

It is BLUE, Best Linear Unbiased Estimtor if we assume

$E(u) = 0; E(u u') = \sigma^2 I$, X is of full rank k

Tutorial 1 will go through this

### Week 2 / Lecture 3

We can look at this like the a conditional distribution and we are interested in the distribution of Y conditional on X. The paramaters of this joint distribution is $\theta_c$ or $\theta$, there are also marginal parameters ietc and those of the joint distribution $\theta_j$ but we are interested in Y conditional on X. 

In this instance exogeneity is X is independent of Y, not u in this case here

We are interested in the first two moments of this conditional expecpecation, $E(Y_t|X_t; \theta_c)$ in linear regression this is the the regression coefficients and the variance of this so $\theta_c = \theta = (\beta, \sigma^2)$

See the example of the join normal distribution. Here of course we are making explicit assumptions on the distribution of the variables

$E(Y_t|X_t) = \mu_y + (\Sigma_{yx} \Sigma^{-1}_{xx})(X_t - \mu_x)$

*Note* find some more notes on this. 

Maximum Likelihood:

give a set of observations, what is the parameter that maximises the likelihood, or maximises the probablilty that these observations come from a given distribution. how likely is the parameter given we have observed these events.
$L(\theta) = f(y_1, \theta)f(y_2, \theta)f(y_3, \theta)$, so what value of theta maximises this. If we take the logs we can sum this so we take the logs and this is called the log-likelihood.

Of course we need to know what the functional form is of our probaility densitve function

So we differentiate our log-likelihood, this is called or score vector, set it to 0, and solve for $\hat\theta$. We then take the second derivative and if it is a negative definite then it is a maximum. 

The information matrix is the negative expectation of this second derivative. 
We also have the average information matrix. 

The Carmer Rao lower bound is 

$V(\hat\theta) >= I(\hat\theta)$, The ML estimator is consistent, as T goes to infinity the estimate converges on the actual value. 

When we look at asymptotic distributions we scale by $\sqrt(T)$

Small second derivative, means we are not sure exactly where our solution is, move up or down and not much changes, larger second derivative means the slope is steeper, so we can pinpoint our solution better, remember first derivative is slope and second derivative is how steep that slope is or how it changes. 

The ML estimator is asymptotically normally distributed and is efficient asymptotically

Some ML estimators are unbiased and some are biased. Unbiasedness is not a property.

Logit or log odds,

Non Linear estimation 

Gradient descent methods to try and find a solution for the 

### Week 2/Lecture 4

ML as applied to the LRM 

We look at the normal distribution, that is we assume our observations are drawn from 
a normal distribution with mean 0 and variance $\sigma^2$

$L(\beta, \sigma^2) = (2 \pi \sigma^2)^{-T/2} e^{u'u/2 \sigma^2}$

First of all we take the derivative wrt to beta to get the score vector, then we take 
the derivative wrt to $\sigma ^2$ NOT $\sigma$, this is key

So out $LL(\beta, \sigma^2) = = -T/2 log(2\pi) - T/2 log(\sigma^2) - 1/s\sigma^2 u'u$

$u = y - X\beta$ so we can take the derivative of this like we did before, see above for u'u

So if we have the MLL we can test AIC and BIC, so good way of comparing models

You cannot compare models thave have different transformations on the dependent variable. e.g log(y) vs y

The end of this lecture looks at transformations and how to interpret regression coefficients, this is very important. Knowing AIC will be important for the exam and how to interpret coefficients


*Questions*

Why is $E[u' u ] = T \sigma^2$

### Week 3/Tutorial 

Some observations: Walter works with the average likelikehood or average log likelihood. This is because sample averages of iid random variables obey certain limit laws like law of large numbers and central limit theorem. However maximising any of these does not make a difference as you will always get the same maximiser. But Walter works with average log likelihood and Ron works with log likelihood

If the hessian is large, then the peak is sharp

Sharper peak (remember second derivative is slope of slope, first derivative gives the slope of the function then second derivative tells us how steep that slope is or if it is going up or down and by how much), means bigger information matrix (in a positive definite sense), which gives us a better estimate. A smaller Information matrix (in a psd sense) means a less clear solution, smaller peak, so harder to determine our solution for our estimate, leading to higher variance. 

Bigger information matrix, smaller variance-covariance matrix of our estimator. .
  
Asymptotic distribution of MLE is normal

Safe thing to do is to use the average 



### Week 3/Lecture 6A

Asymptotic Test 

We look at the unrestricted estimates for the MLE




























